{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import conv2d\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class reconstruct_CNN(nn.Module):\n",
    "    def __init__(self, num_neuron):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "\n",
    "        hidden_dims = [16, 64, 128, 64, 16]\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                               hidden_dims[-1],\n",
    "                               kernel_size=5,\n",
    "                               stride=1,\n",
    "                               padding=2,\n",
    "                               output_padding=2,\n",
    "                               dilation=5),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=1,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.Tanh())\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        self.linear_input = nn.Linear(num_neuron, hidden_dims[0] * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_input(x)\n",
    "        x = x.view(-1, 16, 2, 2)\n",
    "        x = self.layers(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "class selected_rsp_dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, rsps, cifarset:Dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.cifarset = cifarset\n",
    "        self.rsps = rsps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rsps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.cifarset.__getitem__(idx)\n",
    "        rsp = torch.FloatTensor(self.rsps[idx])\n",
    "        return img, rsp\n",
    "\n",
    "\n",
    "def process_img_batch(imgs, filters):\n",
    "    s = filters.shape[2]\n",
    "    outer_size = (50 - s) // 2\n",
    "    image_center = imgs[:, :, outer_size: s + outer_size, outer_size: s + outer_size]\n",
    "    sparse_rsp = conv2d(image_center, filters)\n",
    "    return torch.reshape(sparse_rsp, (len(sparse_rsp), filter_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.functional.mse_loss\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize(50),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Grayscale()]\n",
    ")\n",
    "\n",
    "batch_size = 1024\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "test_rsp = np.load('279_selected_rsp_cifar10_test.npy')\n",
    "\n",
    "testset_rsp = selected_rsp_dataset(test_rsp, testset)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset_rsp, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "test_rsp_15 = np.load('few_selected_rsp_cifar10_test_crop_15.npy')\n",
    "\n",
    "testset_rsp_15 = selected_rsp_dataset(test_rsp_15, testset)\n",
    "\n",
    "testloader_15 = torch.utils.data.DataLoader(testset_rsp_15, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "test_rsp_22 = np.load('few_selected_rsp_cifar10_test_22.npy')\n",
    "\n",
    "testset_rsp_22 = selected_rsp_dataset(test_rsp_22, testset)\n",
    "\n",
    "testloader_22 = torch.utils.data.DataLoader(testset_rsp_22, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "templates = np.load(\"Bruno_BASIS1_NUM_512_size16.npy\")\n",
    "templates = np.transpose(templates)\n",
    "filter_size = np.round(np.sqrt(templates.shape[1])).__int__()\n",
    "filter_num = templates.shape[0]\n",
    "templates = np.reshape(templates, (filter_num, 1, filter_size, filter_size))\n",
    "templates = torch.tensor(templates).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "reconstruct_CNN(\n  (final_layer): Sequential(\n    (0): ConvTranspose2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=(5, 5), output_padding=(2, 2))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): Tanh()\n  )\n  (layers): Sequential(\n    (0): Sequential(\n      (0): ConvTranspose2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (1): Sequential(\n      (0): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (2): Sequential(\n      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (3): Sequential(\n      (0): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n  )\n  (linear_input): Linear(in_features=512, out_features=64, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prednet_15 = reconstruct_CNN(test_rsp_15.shape[1])\n",
    "prednet_15.load_state_dict(torch.load('artificial_recon_model_selective_few_15'))\n",
    "prednet_15.eval().to(device)\n",
    "\n",
    "prednet = reconstruct_CNN(test_rsp.shape[1])\n",
    "prednet.load_state_dict(torch.load('artificial_recon_model_selective_279'))\n",
    "prednet.eval().to(device)\n",
    "\n",
    "prednet_22 = reconstruct_CNN(test_rsp_15.shape[1])\n",
    "prednet_22.load_state_dict(torch.load('artificial_recon_model_selective_few_22'))\n",
    "prednet_22.eval().to(device)\n",
    "\n",
    "prednet_s = reconstruct_CNN(templates.shape[0]).to(device)\n",
    "prednet_s.load_state_dict(torch.load('filter_recon_model_16'))\n",
    "prednet_s.eval().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def in_out_correlation(pred, origin, size):\n",
    "\n",
    "    edge_size = (50 - size) // 2\n",
    "    in_pred = pred[...,edge_size:edge_size + size, edge_size:edge_size + size]\n",
    "    in_origin = origin[...,edge_size:edge_size + size, edge_size:edge_size + size]\n",
    "    in_pred = torch.flatten(in_pred, start_dim=1)\n",
    "    in_origin = torch.flatten(in_origin, start_dim=1)\n",
    "\n",
    "    in_corr = torch.tensor([torch.corrcoef(torch.stack([in_pred[i],in_origin[i]]))[0,1] for i in range(in_pred.shape[0])])\n",
    "\n",
    "    border_index_x = []\n",
    "    border_index_y = []\n",
    "    for i in range(50):\n",
    "        for j in range(50):\n",
    "            if i < edge_size or i >= edge_size + size or j < edge_size or j >= edge_size + size:\n",
    "                border_index_x.append(i)\n",
    "                border_index_y.append(j)\n",
    "    border_index = np.array([np.array(border_index_x), np.array(border_index_y)])\n",
    "    out_pred =  pred[..., border_index[0],border_index[1]]\n",
    "    out_origin =  origin[..., border_index[0],border_index[1]]\n",
    "    out_pred = torch.flatten(out_pred, start_dim=1)\n",
    "    out_origin = torch.flatten(out_origin, start_dim=1)\n",
    "\n",
    "    out_corr = torch.tensor([torch.corrcoef(torch.stack([out_pred[i],out_origin[i]]))[0,1] for i in range(in_pred.shape[0])])\n",
    "    return in_corr, out_corr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_img_corr(loader,prednet, size):\n",
    "    with torch.no_grad():\n",
    "        all_in_corr, all_out_corr = [],[]\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = y\n",
    "            recon = prednet(rsp)\n",
    "            in_corr, out_corr = in_out_correlation(recon, x, size)\n",
    "            #loss = criterion(recon, x)\n",
    "            all_in_corr.append(in_corr)\n",
    "            all_out_corr.append(out_corr)\n",
    "        all_out_corr = torch.concat(all_out_corr)\n",
    "        all_out_corr = all_out_corr.detach().cpu().numpy()\n",
    "        all_in_corr = torch.concat(all_in_corr)\n",
    "        all_in_corr = all_in_corr.detach().cpu().numpy()\n",
    "        return all_out_corr, all_in_corr\n",
    "\n",
    "def get_img_corr_sparse_coding(loader,prednet, size):\n",
    "    with torch.no_grad():\n",
    "        all_in_corr, all_out_corr = [],[]\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = process_img_batch(x, templates)\n",
    "            recon = prednet(rsp)\n",
    "            in_corr, out_corr = in_out_correlation(recon, x, size)\n",
    "            #loss = criterion(recon, x)\n",
    "            all_in_corr.append(in_corr)\n",
    "            all_out_corr.append(out_corr)\n",
    "        all_out_corr = torch.concat(all_out_corr)\n",
    "        all_out_corr = all_out_corr.detach().cpu().numpy()\n",
    "        all_in_corr = torch.concat(all_in_corr)\n",
    "        all_in_corr = all_in_corr.detach().cpu().numpy()\n",
    "        return all_out_corr, all_in_corr\n",
    "\n",
    "def get_img_mse(loader,prednet):\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        all_mse = []\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = y\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "            all_mse.append(loss)\n",
    "        all_mse = torch.concat(all_mse)\n",
    "        all_mse = all_mse.detach().cpu().numpy()\n",
    "        return all_mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "o_c, i_c = get_img_corr(testloader, prednet, 16)\n",
    "o_c1, i_c1 = get_img_corr(testloader, prednet, 22)\n",
    "o_c_15, i_c_15 = get_img_corr(testloader_15, prednet_15, 16)\n",
    "o_c_22, i_c_22 = get_img_corr(testloader_22, prednet_22, 22)\n",
    "o_c_s, i_c_s = get_img_corr_sparse_coding(testloader, prednet_s, 16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "mse = get_img_mse(testloader, prednet)\n",
    "mse_15 = get_img_mse(testloader_15, prednet_15)\n",
    "mse_22 = get_img_mse(testloader_22, prednet_22)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(o_c, alpha = 0.7, label='3 degree')\n",
    "plt.hist(o_c_15,alpha = 0.7, label='1 degree')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/outer_corr_size_50_16\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(o_c1, alpha = 0.7, label='3 degree')\n",
    "plt.hist(o_c_22,alpha = 0.7, label='1.5 degree')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/outer_corr_size_50_22\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(o_c, alpha = 0.7, label='3 degree')\n",
    "plt.hist(o_c_s,alpha = 0.7, label='1 degree sparse_coding')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/outer_corr_size_50_sparse\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(i_c, alpha = 0.7, label='3 degree')\n",
    "plt.hist(i_c_15,alpha = 0.7, label='1 degree')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/inner_corr_size_50_16\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(i_c1, alpha = 0.7, label='3 degree')\n",
    "plt.hist(i_c_22,alpha = 0.7, label='1.5 degree')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/inner_corr_size_50_22\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(i_c, alpha = 0.7, label='3 degree')\n",
    "plt.hist(i_c_s,alpha = 0.7, label='1 degree sparse_coding')\n",
    "plt.ylabel('Image count')\n",
    "plt.xlabel('Reconstruction correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"bar_chart_corr_performance/inner_corr_size_50_sparse\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(o_c, alpha = 0.7, label='50')\n",
    "plt.hist(o_c_15,alpha = 0.7, label='16')\n",
    "plt.hist(o_c_22,alpha = 0.7, label='22')\n",
    "plt.hist(o_c_s,alpha = 0.7, label='sparse_coding')\n",
    "plt.ylabel('neurons count')\n",
    "plt.xlabel('correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"outer_corr_size_22\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(i_c, alpha = 0.7,label='50')\n",
    "plt.hist(i_c_15, alpha = 0.7,label='16')\n",
    "plt.hist(i_c_22,alpha = 0.7, label='22')\n",
    "plt.hist(i_c_s,alpha = 0.7, label='sparse_coding')\n",
    "plt.ylabel('neurons count')\n",
    "plt.xlabel('correlation')\n",
    "plt.legend()\n",
    "plt.savefig(\"inner_corr_size_22\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "mse = np.average(mse, axis=(1,2,3))\n",
    "mse_15 = np.average(mse_15, axis=(1,2,3))\n",
    "mse_22 = np.average(mse_22, axis=(1,2,3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06796965\n",
      "0.16327208\n",
      "0.13623653\n"
     ]
    }
   ],
   "source": [
    "print(np.average(mse))\n",
    "print(np.average(mse_15))\n",
    "print(np.average(mse_22))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(mse)\n",
    "plt.figure()\n",
    "plt.hist(mse_15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "selected_imgs = [50, 85, 98, 149, 188, 198, 245, 315, 331, 429]\n",
    "img_list = []\n",
    "for i in selected_imgs                                             :\n",
    "    origin, sample_full = testset_rsp.__getitem__(i)\n",
    "    recon = prednet(sample_full.to(device)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    _, sample_16 = testset_rsp_15.__getitem__(i)\n",
    "    sample_16.to(device)\n",
    "    recon_16 = prednet_15(sample_16.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    _, sample_22 = testset_rsp_22.__getitem__(i)\n",
    "    sample_22.to(device)\n",
    "    recon_22 = prednet_15(sample_22.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    sample = torch.reshape(origin, (1,1,50,50)).to(device)\n",
    "    recon_s = prednet_s(process_img_batch(sample, templates)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    origin = origin.numpy()\n",
    "    all_imgs = np.array([np.reshape(origin,(50,50)), np.reshape(recon,(50,50)), np.reshape(recon_16,(50,50)),np.reshape(recon_22,(50,50)), np.reshape(recon_s,(50,50))])\n",
    "    img_list.append(all_imgs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "img_list = np.transpose(np.array(img_list), (1,0,2,3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import imshowtools\n",
    "from cv2 import imwrite\n",
    "\n",
    "def show_imgs_in1Page(img_matrix,cmap='gray',showsize=(10,10),columns=None,rows=None,padding=False,title=None):\n",
    "    '''\n",
    "    shape: (numbers,H,W)\n",
    "    '''\n",
    "\n",
    "    assert len(img_matrix.shape)==3\n",
    "    assert isinstance(showsize,tuple)\n",
    "\n",
    "    return imshowtools.imshow(*img_matrix,cmap=cmap,size=showsize,columns=columns,rows=rows,padding=padding,title=title,return_image=True)\n",
    "\n",
    "\n",
    "img = show_imgs_in1Page(np.reshape(img_list,(50,50,50)), cmap='gray',showsize=(50,50),columns=10,rows=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imwrite(\"sample_recon_img_1.png\", img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "reconstruct_CNN(\n  (final_layer): Sequential(\n    (0): ConvTranspose2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=(5, 5), output_padding=(2, 2))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): Tanh()\n  )\n  (layers): Sequential(\n    (0): Sequential(\n      (0): ConvTranspose2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (1): Sequential(\n      (0): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (2): Sequential(\n      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n    (3): Sequential(\n      (0): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.01)\n    )\n  )\n  (linear_input): Linear(in_features=299, out_features=64, bias=True)\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prednet_real = reconstruct_CNN(299)\n",
    "prednet_real.load_state_dict(torch.load('recon_model_tang_data'))\n",
    "prednet_real.eval().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from modeling.train_utils import  array_to_dataloader\n",
    "site = 'm2s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_'+site+'.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_'+site+'.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_'+site+'.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_'+site+'.npy')\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "val_x = np.transpose(val_x, (0, 3, 1, 2))\n",
    "train_loader = array_to_dataloader(train_x, train_y, batch_size=1024, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x, val_y, batch_size=1024)\n",
    "criterion = torch.nn.MSELoss()\n",
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018942605704069138\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_losses = []\n",
    "    prednet_real.to(device)\n",
    "    for i, (x, y) in enumerate(val_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        recon = prednet_real(y)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    print(avg_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = torch.tensor(val_y[:20], dtype=torch.float).to(device)\n",
    "recon = prednet_real(sample).detach().cpu().numpy()\n",
    "origin = val_x[:20]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_tang/recon_{i}.png',r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_tang/origin_{i}.png',img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
