{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from modeling.models.bethge import BethgeModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from modeling.train_utils import array_to_dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import conv2d\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'\n",
    "# TODO: fine tun with validation set?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR tests using shared core"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize(50),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Grayscale()]\n",
    ")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 256\n",
    "num_layers = 9\n",
    "input_size = 50\n",
    "\n",
    "output_size = 299\n",
    "first_k = 9\n",
    "later_k = 3\n",
    "pool_size = 2\n",
    "factorized = True\n",
    "\n",
    "num_maps = 1\n",
    "\n",
    "net = BethgeModel(channels=channels, num_layers=num_layers, input_size=input_size,\n",
    "                  output_size=output_size, first_k=first_k, later_k=later_k,\n",
    "                  input_channels=1, pool_size=pool_size, factorized=True,\n",
    "                  num_maps=num_maps).cuda()\n",
    "\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load('../saved_models/new_learned_models/m2s1_9_model_version_0'))\n",
    "#net.load_state_dict(torch.load(f'../saved_models/cropped_models/m2s1_size_{input_size}_model'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class reconstruct_CNN(nn.Module):\n",
    "    def __init__(self, num_neuron):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "\n",
    "        hidden_dims = [16, 64, 128, 64, 16]\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding= 1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                       hidden_dims[-1],\n",
    "                                       kernel_size=5,\n",
    "                                       stride=1,\n",
    "                                       padding=2,\n",
    "                                       output_padding=2,\n",
    "                                       dilation=5),\n",
    "                    nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                              kernel_size= 3, padding= 1),\n",
    "                    nn.Tanh())\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        self.linear_input = nn.Linear(num_neuron, hidden_dims[0] * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_input(x)\n",
    "        x = x.view(-1, 16, 2, 2)\n",
    "        x = self.layers(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.functional.mse_loss\n",
    "\n",
    "network = net.to(device)\n",
    "network = network.eval()\n",
    "prednet = reconstruct_CNN(299).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "losses = []\n",
    "accs = []\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [01:55<3:09:51, 115.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : train loss is 0.14625312180817127\n",
      "epoch 0 : val loss is   0.11408415392041206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [03:50<3:07:59, 115.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : train loss is 0.1092441799044609\n",
      "epoch 1 : val loss is   0.1111518020182848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [05:45<3:06:16, 115.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 : train loss is 0.1014069500118494\n",
      "epoch 2 : val loss is   0.10031491592526436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [07:38<3:02:39, 114.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 : train loss is 0.09681678412854672\n",
      "epoch 3 : val loss is   0.09602990813553333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [09:30<2:59:29, 113.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 : train loss is 0.09399545885622501\n",
      "epoch 4 : val loss is   0.09459649935364724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [11:22<2:56:55, 112.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 : train loss is 0.09179045873880386\n",
      "epoch 5 : val loss is   0.0910287506878376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [13:14<2:54:42, 112.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 : train loss is 0.0901765878200531\n",
      "epoch 6 : val loss is   0.09155401557683945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [15:06<2:52:42, 112.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 : train loss is 0.08864386230707169\n",
      "epoch 7 : val loss is   0.08789640225470066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [16:55<3:14:42, 126.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     28\u001B[0m         recon \u001B[38;5;241m=\u001B[39m prednet(rsp)\n\u001B[0;32m     29\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(recon, x)\n\u001B[1;32m---> 31\u001B[0m         val_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     32\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(val_losses)\n\u001B[0;32m     33\u001B[0m accs\u001B[38;5;241m.\u001B[39mappend(avg_loss)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "crop = transforms.CenterCrop(input_size)\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = network(crop(x))\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = network(crop(x))\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"sanity_check_m2s1_model\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07913384726271033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    val_losses = []\n",
    "    prednet = prednet.eval()\n",
    "    for i, (x, y) in enumerate(testloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = network(x)\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    print(avg_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prednet.load_state_dict(torch.load('artificial_recon_model_cropped_m2s1_35'))\n",
    "network.load_state_dict(torch.load('../saved_models/cropped_models/m2s1_size_35_model'))\n",
    "network.eval()\n",
    "prednet.eval()\n",
    "for i in range(100):\n",
    "    sample,_ = testset.__getitem__(i)\n",
    "    sample = torch.reshape(sample.to(device), (1,1,50,50))\n",
    "    recon = prednet(network(sample)).detach().cpu().numpy()\n",
    "    origin = sample.detach().cpu().numpy()\n",
    "\n",
    "    r_img = np.reshape(recon, (50, 50))\n",
    "    img = np.reshape(origin, (50, 50))\n",
    "    plt.imsave(f'recon_artificial_m2s1_35/recon_{i}.png', r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_artificial_m2s1_35/origin_{i}.png', img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tang data reconstruction with shared core"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "site = 'm3s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_'+site+'.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_'+site+'.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_'+site+'.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_'+site+'.npy')\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "val_x = np.transpose(val_x, (0, 3, 1, 2))\n",
    "train_loader = array_to_dataloader(train_x, train_y, batch_size=1024, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x, val_y, batch_size=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:07<11:44,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : train loss is 0.0562766996877534\n",
      "epoch 0 : val loss is   0.03410699591040611\n"
     ]
    }
   ],
   "source": [
    "prednet = reconstruct_CNN(324).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        #rsp = network(x)\n",
    "        rsp = y\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            #rsp = network(x)\n",
    "            rsp = y\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"real_recon_model\")\n",
    "        bestloss = avg_loss\n",
    "    torch.save(prednet.state_dict(), \"real_recon_model_accumulative\")\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "site = 'm2s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_'+site+'.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_'+site+'.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_'+site+'.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_'+site+'.npy')\n",
    "sample = torch.tensor(val_x[:10], dtype=torch.float).to(device)\n",
    "sample = torch.reshape(sample, (sample.shape[0],1,50,50))\n",
    "prednet.load_state_dict(torch.load('artificial_recon_model_tang_data'))\n",
    "prednet = prednet.to(device)\n",
    "recon = prednet(network(sample)).detach().cpu().numpy()\n",
    "origin = val_x[:10]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_tang/recon_{i}.png',r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_tang/origin_{i}.png',img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tang data reconstruction with sparse coding rsp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "sparse_coding_dict = np.load(\"all_cell_dict_.npy\", allow_pickle=True)[()]\n",
    "sparse_coding_value = np.transpose(np.stack([sparse_coding_dict[x]['best_rsp_'] for x in range(299)]))\n",
    "train_x_new = val_x[:900]\n",
    "val_x_new = val_x[900:]\n",
    "train_y_new = sparse_coding_value[:900]\n",
    "val_y_new = sparse_coding_value[900:]\n",
    "train_loader = array_to_dataloader(train_x_new, train_y_new, batch_size=10, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x_new, val_y_new, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prednet = reconstruct_CNN(299).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "criterion = nn.functional.mse_loss\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        recon = prednet(y)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            recon = prednet(y)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"sparse_coding_recon_model_tang_data\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "site = 'm2s1'\n",
    "sample = torch.tensor(val_y_new[:10], dtype=torch.float).to(device)\n",
    "prednet.load_state_dict(torch.load('sparse_coding_recon_model_tang_data'))\n",
    "prednet = prednet.to(device)\n",
    "recon = prednet(sample).detach().cpu().numpy()\n",
    "origin = val_x_new[:10]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_sparse//recon_{i}.png',r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_sparse/origin_{i}.png',img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR data reconstruction with sparse coding template convolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "templates = np.load(\"Bruno_BASIS1_NUM_512_size16.npy\")\n",
    "templates = np.transpose(templates)\n",
    "filter_size = np.round(np.sqrt(templates.shape[1])).__int__()\n",
    "filter_num = templates.shape[0]\n",
    "templates = np.reshape(templates, (filter_num,1, filter_size, filter_size))\n",
    "templates = torch.tensor(templates).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def process_img_batch(imgs, filters):\n",
    "    s = filters.shape[2]\n",
    "    outer_size = (50-s)//2\n",
    "    image_center = imgs[:,:, outer_size: s+outer_size, outer_size : s+outer_size]\n",
    "    sparse_rsp = conv2d(image_center, filters)\n",
    "    return torch.reshape(sparse_rsp, (len(sparse_rsp), filter_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iterion = nn.functional.mse_loss\n",
    "\n",
    "prednet = reconstruct_CNN(templates.shape[0]).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = process_img_batch(x, templates)\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = process_img_batch(x, templates)\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"filter_recon_model_16\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prednet.load_state_dict(torch.load('filter_recon_model_16'))\n",
    "prednet.train()\n",
    "for i in range(100):\n",
    "    sample,_ = testset.__getitem__(i)\n",
    "    sample = torch.reshape(sample.to(device), (1,1,50,50))\n",
    "    recon = prednet(process_img_batch(sample, templates)).detach().cpu().numpy()\n",
    "    origin = sample.detach().cpu().numpy()\n",
    "\n",
    "    r_img = np.reshape(recon, (50, 50))\n",
    "    img = np.reshape(origin, (50, 50))\n",
    "    plt.imsave(f'recon_sparse_convolve_16/recon_{i}.png', r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_sparse_convolve_16/origin_{i}.png', img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR data reconstruction with CNN learning the filters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "templates = np.load(\"Bruno_BASIS1_NUM_512_size16.npy\")\n",
    "templates = np.transpose(templates)\n",
    "filter_size = np.round(np.sqrt(templates.shape[1])).__int__()\n",
    "filter_num = templates.shape[0]\n",
    "templates = np.reshape(templates, (filter_num, 1, filter_size, filter_size))\n",
    "templates = torch.tensor(templates).to(device)\n",
    "\n",
    "\n",
    "def process_img_batch(imgs, filters):\n",
    "    s = filters.shape[2]\n",
    "    outer_size = (50 - s) // 2\n",
    "    image_center = imgs[:, :, outer_size: s + outer_size, outer_size: s + outer_size]\n",
    "    sparse_rsp = conv2d(image_center, filters)\n",
    "    return torch.reshape(sparse_rsp, (len(sparse_rsp), filter_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
