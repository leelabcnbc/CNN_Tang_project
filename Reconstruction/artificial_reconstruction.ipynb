{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from modeling.models.bethge import BethgeModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from modeling.train_utils import array_to_dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import conv2d\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'\n",
    "# TODO: fine tun with validation set?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR tests using shared core"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize(50),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Grayscale()]\n",
    ")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "BethgeModel(\n  (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv): Sequential(\n    (0): Sequential(\n      (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (6): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (7): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (8): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): Softplus(beta=1, threshold=20)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (fc): Sequential(\n    (0): multiFactorizedLinear(\n      (bank): ModuleList(\n        (0): FactorizedLinear()\n      )\n    )\n    (1): Softplus(beta=1, threshold=20)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 256\n",
    "num_layers = 9\n",
    "input_size = 50\n",
    "\n",
    "output_size = 324\n",
    "first_k = 9\n",
    "later_k = 3\n",
    "pool_size = 2\n",
    "factorized = True\n",
    "\n",
    "num_maps = 1\n",
    "\n",
    "net = BethgeModel(channels=channels, num_layers=num_layers, input_size=input_size,\n",
    "                  output_size=output_size, first_k=first_k, later_k=later_k,\n",
    "                  input_channels=1, pool_size=pool_size, factorized=True,\n",
    "                  num_maps=num_maps).cuda()\n",
    "\n",
    "net.to(device)\n",
    "#net.load_state_dict(torch.load('../saved_models/new_learned_models/m2s1_9_model_version_0'))\n",
    "#net.load_state_dict(torch.load(f'../saved_models/cropped_models/m2s1_size_{input_size}_model'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class reconstruct_CNN(nn.Module):\n",
    "    def __init__(self, num_neuron):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "\n",
    "        hidden_dims = [16, 64, 128, 64, 16]\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding= 1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                       hidden_dims[-1],\n",
    "                                       kernel_size=5,\n",
    "                                       stride=1,\n",
    "                                       padding=2,\n",
    "                                       output_padding=2,\n",
    "                                       dilation=5),\n",
    "                    nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                              kernel_size= 3, padding= 1),\n",
    "                    nn.Tanh())\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        self.linear_input = nn.Linear(num_neuron, hidden_dims[0] * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_input(x)\n",
    "        x = x.view(-1, 16, 2, 2)\n",
    "        x = self.layers(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.functional.mse_loss\n",
    "\n",
    "network = net.to(device)\n",
    "network = network.eval()\n",
    "prednet = reconstruct_CNN(324).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "losses = []\n",
    "accs = []\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [01:55<3:09:51, 115.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : train loss is 0.14625312180817127\n",
      "epoch 0 : val loss is   0.11408415392041206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [03:50<3:07:59, 115.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : train loss is 0.1092441799044609\n",
      "epoch 1 : val loss is   0.1111518020182848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [05:45<3:06:16, 115.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 : train loss is 0.1014069500118494\n",
      "epoch 2 : val loss is   0.10031491592526436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [07:38<3:02:39, 114.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 : train loss is 0.09681678412854672\n",
      "epoch 3 : val loss is   0.09602990813553333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [09:30<2:59:29, 113.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 : train loss is 0.09399545885622501\n",
      "epoch 4 : val loss is   0.09459649935364724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [11:22<2:56:55, 112.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 : train loss is 0.09179045873880386\n",
      "epoch 5 : val loss is   0.0910287506878376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [13:14<2:54:42, 112.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 : train loss is 0.0901765878200531\n",
      "epoch 6 : val loss is   0.09155401557683945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [15:06<2:52:42, 112.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 : train loss is 0.08864386230707169\n",
      "epoch 7 : val loss is   0.08789640225470066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [16:55<3:14:42, 126.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     28\u001B[0m         recon \u001B[38;5;241m=\u001B[39m prednet(rsp)\n\u001B[0;32m     29\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(recon, x)\n\u001B[1;32m---> 31\u001B[0m         val_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     32\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(val_losses)\n\u001B[0;32m     33\u001B[0m accs\u001B[38;5;241m.\u001B[39mappend(avg_loss)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "crop = transforms.CenterCrop(input_size)\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = network(crop(x))\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = network(crop(x))\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"sanity_check_m2s1_model\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07913384726271033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    val_losses = []\n",
    "    prednet = prednet.eval()\n",
    "    for i, (x, y) in enumerate(testloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = network(x)\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    print(avg_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prednet.load_state_dict(torch.load('artificial_recon_model_cropped_m2s1_35'))\n",
    "network.load_state_dict(torch.load('../saved_models/cropped_models/m2s1_size_35_model'))\n",
    "network.eval()\n",
    "prednet.eval()\n",
    "for i in range(100):\n",
    "    sample,_ = testset.__getitem__(i)\n",
    "    sample = torch.reshape(sample.to(device), (1,1,50,50))\n",
    "    recon = prednet(network(sample)).detach().cpu().numpy()\n",
    "    origin = sample.detach().cpu().numpy()\n",
    "\n",
    "    r_img = np.reshape(recon, (50, 50))\n",
    "    img = np.reshape(origin, (50, 50))\n",
    "    plt.imsave(f'recon_artificial_m2s1_35/recon_{i}.png', r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_artificial_m2s1_35/origin_{i}.png', img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tang data reconstruction with shared core"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "site = 'm3s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_'+site+'.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_'+site+'.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_'+site+'.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_'+site+'.npy')\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "val_x = np.transpose(val_x, (0, 3, 1, 2))\n",
    "train_loader = array_to_dataloader(train_x, train_y, batch_size=20, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x, val_y, batch_size=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prednet = reconstruct_CNN(324).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        #rsp = network(x)\n",
    "        recon = prednet(y)\n",
    "        loss = criterion(recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            #rsp = network(x)\n",
    "            recon = prednet(y)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"recon_model_tang_data_m3s1\")\n",
    "        bestloss = avg_loss\n",
    "    torch.save(prednet.state_dict(), \"recon_model_tang_data_m3s1_acc\")\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n"
     ]
    }
   ],
   "source": [
    "site = 'm3s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_'+site+'.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_'+site+'.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_'+site+'.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_'+site+'.npy')\n",
    "sample = torch.tensor(val_x[:10], dtype=torch.float).to(device)\n",
    "sample = torch.reshape(sample, (sample.shape[0],1,50,50))\n",
    "prednet.load_state_dict(torch.load('recon_model_tang_data_m3s1'))\n",
    "prednet = prednet.to(device)\n",
    "recon = prednet(network(sample)).detach().cpu().numpy()\n",
    "origin = val_x[:10]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_tang_m3s1/recon_{i}.png',r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_tang_m3s1/origin_{i}.png',img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Mapper(torch.nn.Module):\n",
    "    def __init__(self, numNeurons):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(numNeurons,512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,numNeurons),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output\n",
    "network = Mapper(299).to(device)\n",
    "network.load_state_dict(torch.load('real_fake_mapper_corr'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "site = 'm2s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_' + site + '.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_' + site + '.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_' + site + '.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_' + site + '.npy')\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "val_x = np.transpose(val_x, (0, 3, 1, 2))\n",
    "train_loader = array_to_dataloader(train_x, train_y, batch_size=20, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x, val_y, batch_size=20)\n",
    "prednet = reconstruct_CNN(299).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.0005)\n",
    "criterion = nn.MSELoss()\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:19<31:24, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : train loss is 0.016884978087809012\n",
      "epoch 0 : val loss is   0.197516151368618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:37<30:55, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : train loss is 0.015148887807997514\n",
      "epoch 1 : val loss is   0.17448889032006265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:58<31:59, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 : train loss is 0.014731904284610431\n",
      "epoch 2 : val loss is   0.15641219809651374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:17<31:06, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 : train loss is 0.014413499657292755\n",
      "epoch 3 : val loss is   0.159585816860199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:36<30:30, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 : train loss is 0.01415679029284083\n",
      "epoch 4 : val loss is   0.16214995682239533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:58<31:39, 20.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 : train loss is 0.01403614969163829\n",
      "epoch 5 : val loss is   0.14965099781751634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [02:18<31:07, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 : train loss is 0.01392604076512614\n",
      "epoch 6 : val loss is   0.1516859546303749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [02:36<29:57, 19.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 : train loss is 0.013826183146055864\n",
      "epoch 7 : val loss is   0.1531093680858612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:55<29:10, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 : train loss is 0.013735271670228364\n",
      "epoch 8 : val loss is   0.14772795543074607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [03:13<28:23, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 : train loss is 0.013686756702634145\n",
      "epoch 9 : val loss is   0.15389274582266807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [03:31<27:40, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 : train loss is 0.013615378696395427\n",
      "epoch 10 : val loss is   0.14789145022630693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [03:49<27:03, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 : train loss is 0.013575011684502265\n",
      "epoch 11 : val loss is   0.15205459594726561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [04:08<26:46, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 : train loss is 0.013519523629196444\n",
      "epoch 12 : val loss is   0.15740030154585838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [04:26<26:23, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 : train loss is 0.013459194891002713\n",
      "epoch 13 : val loss is   0.14648803174495698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [04:44<26:05, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 : train loss is 0.013411694703509613\n",
      "epoch 14 : val loss is   0.15006860703229905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [05:03<25:41, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 : train loss is 0.013369065739062368\n",
      "epoch 15 : val loss is   0.14722641363739966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [05:21<25:19, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 : train loss is 0.013323390930510905\n",
      "epoch 16 : val loss is   0.152011236846447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [05:39<25:02, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 : train loss is 0.013294619105148071\n",
      "epoch 17 : val loss is   0.1569168108701706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [05:58<24:45, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 : train loss is 0.013240778271336944\n",
      "epoch 18 : val loss is   0.1534159579873085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [06:16<24:25, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 : train loss is 0.013214543159785016\n",
      "epoch 19 : val loss is   0.15533978268504142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [06:34<24:11, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 : train loss is 0.013183379894768706\n",
      "epoch 20 : val loss is   0.14783710479736328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [06:53<24:10, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 : train loss is 0.01313333203027747\n",
      "epoch 21 : val loss is   0.16577915132045745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [07:15<24:59, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 : train loss is 0.013115834699639556\n",
      "epoch 22 : val loss is   0.14456997364759444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [07:35<24:52, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 : train loss is 0.013068322925543299\n",
      "epoch 23 : val loss is   0.15192643851041793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [07:56<25:06, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 : train loss is 0.013018881006128327\n",
      "epoch 24 : val loss is   0.1521327744424343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [08:15<24:13, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 : train loss is 0.01299526167786395\n",
      "epoch 25 : val loss is   0.15006964445114135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [08:34<23:36, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 : train loss is 0.012961988986032654\n",
      "epoch 26 : val loss is   0.1544570817053318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [08:52<23:05, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 : train loss is 0.012926981664281719\n",
      "epoch 27 : val loss is   0.1423554600775242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [09:13<23:26, 19.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 : train loss is 0.012899566500884842\n",
      "epoch 28 : val loss is   0.16367422550916672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [09:35<23:31, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 : train loss is 0.012865662151111328\n",
      "epoch 29 : val loss is   0.16267801716923713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [09:57<23:51, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 : train loss is 0.012832391936027882\n",
      "epoch 30 : val loss is   0.16117963656783105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [10:21<24:39, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 : train loss is 0.012817681660989717\n",
      "epoch 31 : val loss is   0.1513191194832325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [10:44<24:48, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 : train loss is 0.012765937614425713\n",
      "epoch 32 : val loss is   0.16833265930414198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [11:05<24:06, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 : train loss is 0.012755642895947914\n",
      "epoch 33 : val loss is   0.16973122850060463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [11:26<23:14, 21.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 : train loss is 0.012713113092076109\n",
      "epoch 34 : val loss is   0.15271683514118195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [11:46<22:30, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 : train loss is 0.012680667340565397\n",
      "epoch 35 : val loss is   0.15491742685437201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [12:06<21:42, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 : train loss is 0.01264244555286607\n",
      "epoch 36 : val loss is   0.16026091679930687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [12:27<21:27, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 : train loss is 0.012616850264864612\n",
      "epoch 37 : val loss is   0.1553737363219261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [12:46<20:46, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 : train loss is 0.012580802507166351\n",
      "epoch 38 : val loss is   0.15760955840349197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [13:06<20:17, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 : train loss is 0.012549446579449031\n",
      "epoch 39 : val loss is   0.16611159458756447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [13:25<19:34, 19.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 : train loss is 0.01252844175228811\n",
      "epoch 40 : val loss is   0.16232905745506288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [13:44<18:53, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 : train loss is 0.012489162767115904\n",
      "epoch 41 : val loss is   0.16786456003785133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [14:04<18:38, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 : train loss is 0.012464137389419639\n",
      "epoch 42 : val loss is   0.16575498521327972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [14:24<18:26, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 : train loss is 0.012443528752583935\n",
      "epoch 43 : val loss is   0.1683898787200451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [14:48<19:21, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 : train loss is 0.012401575743962003\n",
      "epoch 44 : val loss is   0.15121909618377685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [15:08<18:33, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 : train loss is 0.012364584753974056\n",
      "epoch 45 : val loss is   0.16372281193733215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [15:34<19:53, 22.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 : train loss is 0.0123494551368818\n",
      "epoch 46 : val loss is   0.16742963179945947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [16:00<20:16, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 : train loss is 0.012325476827670117\n",
      "epoch 47 : val loss is   0.1651649197936058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [16:26<20:28, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 : train loss is 0.012288669038525954\n",
      "epoch 48 : val loss is   0.17010887801647187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [16:52<20:32, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 : train loss is 0.012264217626759593\n",
      "epoch 49 : val loss is   0.16640994638204576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [17:17<20:23, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 : train loss is 0.012232765871271187\n",
      "epoch 50 : val loss is   0.1768432405591011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [17:37<18:45, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 : train loss is 0.012192739034641763\n",
      "epoch 51 : val loss is   0.16763269320130347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [18:18<22:29, 28.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 : train loss is 0.012186537093806023\n",
      "epoch 52 : val loss is   0.17037574902176858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [18:38<19:52, 25.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 : train loss is 0.012145718615882251\n",
      "epoch 53 : val loss is   0.16600133284926413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [19:14<21:46, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 : train loss is 0.012134793150447765\n",
      "epoch 54 : val loss is   0.16669574201107026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [19:34<19:22, 26.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 : train loss is 0.01209380707646511\n",
      "epoch 55 : val loss is   0.17697576135396959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [20:10<20:52, 29.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 : train loss is 0.01207573827451133\n",
      "epoch 56 : val loss is   0.17279201105237008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [20:30<15:28, 21.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m recon \u001B[38;5;241m=\u001B[39m prednet(rsp)\n\u001B[0;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(recon, x)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = network(y)\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = network(y)\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"recon_model_tang_fakeReal_m2s1\")\n",
    "        bestloss = avg_loss\n",
    "    torch.save(prednet.state_dict(), \"recon_model_tang_fakeReal_m2s1_acc\")\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n",
      "newimg\n"
     ]
    }
   ],
   "source": [
    "site = 'm2s1'\n",
    "train_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/train_img_' + site + '.npy')\n",
    "val_x = np.load('../data/Processed_Tang_data/all_sites_data_prepared/pics_data/val_img_' + site + '.npy')\n",
    "train_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/trainRsp_' + site + '.npy')\n",
    "val_y = np.load('../data/Processed_Tang_data/all_sites_data_prepared/New_response_data/valRsp_' + site + '.npy')\n",
    "sample = torch.tensor(train_y[:20], dtype=torch.float).to(device)\n",
    "prednet.load_state_dict(torch.load('recon_model_tang_fakeReal_m2s1'))\n",
    "prednet = prednet.to(device)\n",
    "recon = prednet(network(sample)).detach().cpu().numpy()\n",
    "origin = train_x[:20]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_tang_m2s1_fakeReal/recon_{i}.png', r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_tang_m2s1_fakeReal/origin_{i}.png', img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tang data reconstruction with sparse coding rsp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "sparse_coding_dict = np.load(\"all_cell_dict_.npy\", allow_pickle=True)[()]\n",
    "sparse_coding_value = np.transpose(np.stack([sparse_coding_dict[x]['best_rsp_'] for x in range(299)]))\n",
    "train_x_new = val_x[:900]\n",
    "val_x_new = val_x[900:]\n",
    "train_y_new = sparse_coding_value[:900]\n",
    "val_y_new = sparse_coding_value[900:]\n",
    "train_loader = array_to_dataloader(train_x_new, train_y_new, batch_size=10, shuffle=True)\n",
    "val_loader = array_to_dataloader(val_x_new, val_y_new, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prednet = reconstruct_CNN(299).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "criterion = nn.functional.mse_loss\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        recon = prednet(y)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            recon = prednet(y)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"sparse_coding_recon_model_tang_data\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "site = 'm2s1'\n",
    "sample = torch.tensor(val_y_new[:10], dtype=torch.float).to(device)\n",
    "prednet.load_state_dict(torch.load('sparse_coding_recon_model_tang_data'))\n",
    "prednet = prednet.to(device)\n",
    "recon = prednet(sample).detach().cpu().numpy()\n",
    "origin = val_x_new[:10]\n",
    "for i, (r_img, img) in enumerate(zip(recon, origin)):\n",
    "    r_img = np.reshape(r_img, (50, 50))\n",
    "    img = np.reshape(img, (50, 50))\n",
    "    print(\"newimg\")\n",
    "    plt.imsave(f'recon_sparse//recon_{i}.png',r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_sparse/origin_{i}.png',img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR data reconstruction with sparse coding template convolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "templates = np.load(\"Bruno_BASIS1_NUM_512_size16.npy\")\n",
    "templates = np.transpose(templates)\n",
    "filter_size = np.round(np.sqrt(templates.shape[1])).__int__()\n",
    "filter_num = templates.shape[0]\n",
    "templates = np.reshape(templates, (filter_num,1, filter_size, filter_size))\n",
    "templates = torch.tensor(templates).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def process_img_batch(imgs, filters):\n",
    "    s = filters.shape[2]\n",
    "    outer_size = (50-s)//2\n",
    "    image_center = imgs[:,:, outer_size: s+outer_size, outer_size : s+outer_size]\n",
    "    sparse_rsp = conv2d(image_center, filters)\n",
    "    return torch.reshape(sparse_rsp, (len(sparse_rsp), filter_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iterion = nn.functional.mse_loss\n",
    "\n",
    "prednet = reconstruct_CNN(templates.shape[0]).to(device)\n",
    "optimizer = torch.optim.Adam(prednet.parameters(), lr=0.005)\n",
    "losses = []\n",
    "accs = []\n",
    "bestloss = 200\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    prednet = prednet.train()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        rsp = process_img_batch(x, templates)\n",
    "        recon = prednet(rsp)\n",
    "        loss = criterion(recon, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        prednet = prednet.eval()\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            rsp = process_img_batch(x, templates)\n",
    "            recon = prednet(rsp)\n",
    "            loss = criterion(recon, x)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    accs.append(avg_loss)\n",
    "    if avg_loss < bestloss:\n",
    "        torch.save(prednet.state_dict(), \"filter_recon_model_16\")\n",
    "        bestloss = avg_loss\n",
    "\n",
    "    print(f'epoch {e} : train loss is {float(losses[-1])}')\n",
    "    print(f'epoch {e} : val loss is   {float(accs[-1])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prednet.load_state_dict(torch.load('filter_recon_model_16'))\n",
    "prednet.train()\n",
    "for i in range(100):\n",
    "    sample,_ = testset.__getitem__(i)\n",
    "    sample = torch.reshape(sample.to(device), (1,1,50,50))\n",
    "    recon = prednet(process_img_batch(sample, templates)).detach().cpu().numpy()\n",
    "    origin = sample.detach().cpu().numpy()\n",
    "\n",
    "    r_img = np.reshape(recon, (50, 50))\n",
    "    img = np.reshape(origin, (50, 50))\n",
    "    plt.imsave(f'recon_sparse_convolve_16/recon_{i}.png', r_img, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imsave(f'recon_sparse_convolve_16/origin_{i}.png', img, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIFAR data reconstruction with CNN learning the filters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "templates = np.load(\"Bruno_BASIS1_NUM_512_size16.npy\")\n",
    "templates = np.transpose(templates)\n",
    "filter_size = np.round(np.sqrt(templates.shape[1])).__int__()\n",
    "filter_num = templates.shape[0]\n",
    "templates = np.reshape(templates, (filter_num, 1, filter_size, filter_size))\n",
    "templates = torch.tensor(templates).to(device)\n",
    "\n",
    "\n",
    "def process_img_batch(imgs, filters):\n",
    "    s = filters.shape[2]\n",
    "    outer_size = (50 - s) // 2\n",
    "    image_center = imgs[:, :, outer_size: s + outer_size, outer_size: s + outer_size]\n",
    "    sparse_rsp = conv2d(image_center, filters)\n",
    "    return torch.reshape(sparse_rsp, (len(sparse_rsp), filter_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
